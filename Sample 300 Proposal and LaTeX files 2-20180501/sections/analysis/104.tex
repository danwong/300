\subsection{Tenet 1.04: Potential Danger}
Tenet 1.04 of the SE Code of Ethics requires software engineers to "\uline{disclose to appropriate persons} ... any \uline{actual or potential danger} to the user ... that they \uline{reasonably believe} to be associated with software or related documents" \cite{code}.

\subsubsection{Definitions}
\subsubsubsection{Disclosure to Appropriate Persons}
To "disclose" is to "make known, reveal, or uncover" \cite{define-disclose}.  In the context of research, disclosure is closely related to informed consent, which the Institutional Review Board Guidebook claims "assures that prospective human subjects will understand the nature of the research and can knowledgeably and voluntarily decide whether or not to participate" \cite{irb-informed-consent}.  This definition regards prospective human subjects as the appropriate persons to which disclosure is directed.  According to these definitions, to "disclose to appropriate persons" in research means to "make known to prospective human subjects (users)", and disclosure should include an opportunity to decide whether or not to participate in the research.
\subsubsubsection{Actual or Potential Danger}
"Danger" is defined as "liability or exposure to harm or injury; risk; peril" \cite{define-danger}.  According to the Research Ethics Guidebook, "harm in social science research includes quite subjective evaluations like distress, embarrassment, and anxiety" \cite{define-harm}.  This definition is relevant because social science is "the study of society and social behavior" \cite{define-social-science}, and Facebook's "emotional contagion" study "tested whether exposure to emotions led people to change their own posting behaviors" \cite{study}.  Therefore, "actual or potential danger" may be defined as "potential risk of distress, embarrassment, or anxiety".
\subsubsubsection{Reasonable Belief}
Reasonable belief may be defined as "hav[ing] knowledge of facts which, although not amounting to direct knowledge, would cause a reasonable person, knowing the same facts, to reasonably conclude the same thing" \cite{define-reasonable-belief}.  In other words, reasonable belief requires evidence, but not necessarily proof.  To "reasonably believe", then, means to "have evidence to believe".

\subsubsection{Domain Specific Rule}
In the domain of internet research, tenet 1.04 requires Facebook's software engineers to "\uline{make known to users} ... any \uline{potential risk of distress, embarrassment, or anxiety} to the user ... that they \uline{have evidence to believe} is associated with \uline{the Facebook news feed algorithm}."

\subsubsection{Discussion}

\subsubsubsection{Potential Risks}
One major defense of Facebook is Tal Yarkoni's "In Defense of Facebook" article, which questions whether the "emotional contagion" experiment was any different from routine updates to Facebook.  Yarkoni argues that "every single change Facebook makes to the site alters the user experience", and that these updates are made in the interest of improving the user experience \cite{defense}.  Michelle Meyer agrees, arguing that the "emotional contagion" experiment was ultimately designed to improve the user experience, and that it did not "mess with people's minds" any more than Facebook usually does \cite{misjudgements}.  \par
It is important to note, however, that the "emotional contagion" experiment was not conducted as a routine update to the system; it was designed to test "whether emotional contagion occurs outside of in-person interaction between individuals" \cite{study}.  Adam Kramer, one of the study's authors, stated that the researchers' "goal was never to upset anyone" \cite{atlantic}.  The intentions of the study, however, are irrelevant to the potential for harm, which may have been necessary for the experiment to yield beneficial results.  Kramer goes on to acknowledge that the study resulted in harm to the subjects: "the research benefits of the paper may not have justified all of [the] \textit{anxiety}" [emphasis added].  Did the researchers have evidence to believe that the changes made for the experiment might cause anxiety before the study was conducted? \par
The paper on the study makes references to previous studies on "emotional contagion": "Emotional contagion is well established in laboratory experiments, with people transferring positive and \textit{negative emotions} to others.  Data from a large real-world social network, collected over a 20-y period suggests that longer-lasting moods (e.g., \textit{depression}, happiness) can be transferred through networks" [emphasis added] \cite{study}.  These references are clear acknowledgments that the phenomenon of "emotional contagion" has been demonstrated to include the spreading of negative emotions (including the anxiety that Kramer acknowledged).  As the study was designed to examine "emotional contagion" via Facebook News Feeds, the researchers must have considered the possibility that negative emotions could be successfully spread during the experiment.  Therefore, the studies cited in the paper constitute sufficient evidence for the potential risk of negative emotions, including anxiety, associated with the software relevant to the experiment. \par

\subsubsubsection{Making Risks Known}
If the researchers had sufficient evidence for potential risk, did they make these risks known to the subjects of the experiment? \par
As previously mentioned, disclosure is an important component of informed consent \cite{irb-informed-consent}.  The researchers state that because they did not personally see any user data, the experiment "was consistent with Facebookâ€™s Data Use Policy, to which all users agree prior to creating an account on Facebook, constituting informed consent" \cite{study}.  James Grimmelmann agrees that this is "a meaningful way of avoiding privacy harms", which is a "principle risk" in observational studies \cite{laboratorium}.  However, he goes on to point out that the "emotional contagion" study was an experimental study, which carries more potential risks than just privacy risks.  Indeed, the paper does not directly address other potential risks associated with informed consent, including the risk of distress, embarrassment, or anxiety. \par
If the researchers claim that Facebook's Data Use Policy constitutes informed consent, does it also address these risks?  The policy states that Facebook "may use the information [it] receive about [users]: ... for internal operations, including troubleshooting, data analysis, testing, \textit{research} and service improvement" [emphasis added] \cite{leeper}.  However, Kashmir Hill points out that the term "research" was added four months after the study was conducted \cite{forbes}.  This means that the Data Use Policy could not have been considered notification of participation in the study at the time it was conducted.  As many critics point out, Facebook did not provide any explicit notification of participation in the study, and it was conducted without the subjects' knowledge \cite{atlantic} \cite{forbes} \cite{leeper} \cite{slate}.  Therefore, the subjects could not have been aware of any potential risks associated with the experiment, since they were not even aware that they were subjects of an experiment in the first place.

\subsubsection{Conclusion}
The researchers behind Facebook's "emotional contagion" study had evidence to believe in potential risks of anxiety to human subjects associated with the software used in the experiment.  The domain specific rule above, derived from tenet 1.04 of the Software Engineering Code of Ethics, states that they were required to make these risks known to the participants of the experiment.  Because participation in the study was not made known to the subjects, they could not have been aware of these potential risks, and this rule was not satisfied.
\vspace{0.2cm}